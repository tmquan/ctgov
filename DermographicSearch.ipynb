{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 11:17:48.562685: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 11:17:49.049284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from txtai.embeddings import Embeddings\n",
    "\n",
    "\n",
    "import geopy.distance\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'cache'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x: x\n",
    "    \n",
    "from txtai.embeddings import Embeddings\n",
    "from txtai.pipeline import Similarity\n",
    "from txtai.pipeline import Tabular\n",
    "from txtai.workflow import Task\n",
    "from txtai.workflow import Workflow\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'cache'\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(x): return x\n",
    "\n",
    "\n",
    "class SemanticSearch(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename=\"ctgov_34983_20230417\",\n",
    "        columns=[\n",
    "            \"brief_title\",\n",
    "            \"official_title\",\n",
    "            \"brief_summaries\",\n",
    "            \"detailed_descriptions\",\n",
    "            \"criteria\",\n",
    "            \"city\", \"state\", \"zip\", \"country\"\n",
    "        ],\n",
    "        ckptlist=[\n",
    "            \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "        ],\n",
    "        rerun=True,\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.columns = columns\n",
    "        self.ckptlist = ckptlist\n",
    "\n",
    "        for ckptpath in self.ckptlist:\n",
    "            snapshot_download(repo_id=ckptpath,\n",
    "                              repo_type=\"model\",\n",
    "                              cache_dir=\"cache\")\n",
    "            self.embeddings = Embeddings({\n",
    "                \"method\": \"transformers\",\n",
    "                \"path\": ckptpath,\n",
    "                \"content\": True,\n",
    "                \"object\": True\n",
    "            })\n",
    "            indexfile = f'{filename}_{ckptpath.replace(\"/\", \"-\")}.index'\n",
    "            if os.path.exists(indexfile) and rerun is False:\n",
    "                print(\"Indexed and Cached!\")\n",
    "                self.embeddings.load(indexfile)\n",
    "            else:\n",
    "                print(\"Need to rerun or Indices and Caches dont exist, run them!\")\n",
    "\n",
    "                # Read the data from CSV\n",
    "                data = pd.read_csv(f'{filename}.csv')\n",
    "\n",
    "                # Create tabular instance mapping input.csv fields\n",
    "                tabular = Tabular(idcolumn=\"nct_id\",\n",
    "                                  textcolumns=columns, content=True)\n",
    "\n",
    "                # Create workflow\n",
    "                workflow = Workflow([Task(tabular)])\n",
    "\n",
    "                # Index the data\n",
    "                data = list(workflow([data]))\n",
    "                self.embeddings.index(data)\n",
    "                self.embeddings.save(indexfile)\n",
    "                print(\"Indexing and Caching finished for the 1st time!\")\n",
    "\n",
    "    def search_func(self, \n",
    "                    prompttext, \n",
    "                    pretrained=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\", \n",
    "                    limit=10):\n",
    "        assert pretrained in self.ckptlist\n",
    "        query = f'select {\", \".join([\"nct_id\"] + [column for column in self.columns])} from txtai where similar({prompttext})'\n",
    "        results = self.embeddings.search(query, limit)\n",
    "        return results\n",
    "        \n",
    "    def search_cond(self, \n",
    "                    results, \n",
    "                    location, \n",
    "                    distance):\n",
    "        \n",
    "        # Parse location into latitude and longitude using geopy\n",
    "        geolocator = Nominatim(user_agent=\"my-app\")\n",
    "        location_obj = geolocator.geocode(location)\n",
    "        if location_obj is None:\n",
    "            raise ValueError(f\"Could not find location: {location}\")\n",
    "        location_coords = (location_obj.latitude, location_obj.longitude)\n",
    "\n",
    "        # Filter results based on distance from the location\n",
    "        filtered_results = []\n",
    "        for result in results:\n",
    "            nct_id = result['nct_id']\n",
    "            # Use the first location column available\n",
    "            location_col = next((col for col in result.keys() if col in [\n",
    "                                'city', 'state', 'zip', 'country']), None)\n",
    "            if location_col is None:\n",
    "                # No location column found, skip this result\n",
    "                continue\n",
    "            location_str = result[location_col]\n",
    "            location_obj = geolocator.geocode(location_str)\n",
    "            if location_obj is None:\n",
    "                # Could not parse location, skip this result\n",
    "                continue\n",
    "            result_coords = (location_obj.latitude, location_obj.longitude)\n",
    "            dist = geopy.distance.distance(location_coords, result_coords).km\n",
    "            if dist <= distance:\n",
    "                filtered_results.append(result)\n",
    "\n",
    "        return filtered_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de80f42f5f44a21aebeb431da4a73ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed and Cached!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trial_search = SemanticSearch(\n",
    "    filename=\"ctgov_34983_20230417\",\n",
    "    columns=[\n",
    "        \"brief_title\",\n",
    "        \"official_title\",\n",
    "        \"brief_summaries\",\n",
    "        \"detailed_descriptions\",\n",
    "        \"criteria\",\n",
    "        \"city\",\n",
    "        \"state\",\n",
    "        \"zip\",\n",
    "        \"country\",\n",
    "    ],\n",
    "    ckptlist=[\n",
    "        \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    ],\n",
    "    rerun=False,\n",
    ")\n",
    "results = trial_search.search_func(prompttext=\"diabetes\", limit=100)\n",
    "display(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"Kendall MIT\", \"Seatle\", \"San Francisco\"]\n",
    "for location in locations:\n",
    "    filtered_results = trial_search.search_cond(\n",
    "        results, \n",
    "        location=location, \n",
    "        distance=100)\n",
    "    display(filtered_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
